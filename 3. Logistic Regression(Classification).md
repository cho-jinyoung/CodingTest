## lec05-1. Logistic Classification의 가설 함수 정의
- Binary Classification(이항분류)
> ex) spam detection: spam(1) or Ham(0)  
>  Facebook feed: show(1) or hide(0)
- Logistic regression classification의 가설함수(Hypothesis)  
  -z=WX, H(x)=g(z) (0<g(z)<1)
![시그모이드함수]https://mblogthumb-phinf.pstatic.net/20160426_162/cattree_studio_1461614301227SqDwM_PNG/SigmoidGraph.png?type=w800
![image](https://user-images.githubusercontent.com/54131109/74749379-6c6b1b80-52ad-11ea-8fc6-dc528cd7f652.png)  
## lec05-2. Logistic Regression의 cost함수 설명  
- New cost function for logistic  
> ⅰ) y=1, H(x)이 1이 나와야 하므로 1에서 멀어질수록 cost값이 커지고 1이 나오면 cost=0  
> ⅱ) y=0, H(x)이 0이 나와야 하므로 0에서 멀어질수록 cost값이 커지고 0이 나오면 cost=0  
![image](https://user-images.githubusercontent.com/54131109/74749942-46924680-52ae-11ea-8489-a44c0af8e06f.png)
> ⅰ) y=1, -log(H(x)) x값이 1이면 y=0이고 x값이 0에 가까워 지면 y값은 무한에 가까워 짐  
> ⅱ) y=0, -log(1-H(x)) x값이 0이면 y=0이고 x값이 1에 가까워 지면 y값은 무한에 가까워 짐  
> → y값이 cost값이므로 비용 즉 cost값이 최소가 되어야 하므로 y값이 무한에 가까워지는 값은 선택할 수 없음
![image](https://user-images.githubusercontent.com/54131109/74914384-b2d38e00-5405-11ea-90e8-25da554d6c68.png)
- 위의 수식을 경우의 수 없이 하나의 식으로 수식 수정  
![image](https://user-images.githubusercontent.com/54131109/74750815-b05f2000-52af-11ea-845d-c8536f26cfe2.png)  
```python
# cost function
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis)))
```
- Minimize cost-Gradient decent algorithm  
![image](https://user-images.githubusercontent.com/54131109/74751724-097b8380-52b1-11ea-9ec1-ed3048a11aef.png)
```python
a = tf.Variable(0.1)  # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```
## lab05. Tensorflow로 Logistic Classification 구현하기
-training data
```python
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
# placeholders for a tensor that will be always fed.
X = tf.placeholder(tf.float32, shape=[None, 2])
Y = tf.placeholder(tf.float32, shape=[None, 1])
```
```python
W = tf.Variable(tf.random_normal([2, 1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')
# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))
hypothesis = tf.sigmoid(tf.matmul(X, W) + b)
# cost/loss function
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
# Accuracy computation
# True if hypothesis>0.5 else False
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))
```
- train the model
```
# Launch graph
with tf.Session() as sess:
   # Initialize TensorFlow variables
   sess.run(tf.global_variables_initializer())
   for step in range(10001):
       cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
       if step % 200 == 0:
           print(step, cost_val)
   # Accuracy report
   h, c, a = sess.run([hypothesis, predicted, accuracy],
                      feed_dict={X: x_data, Y: y_data})
   print("\nHypothesis: ", h, "\nCorrect (Y): ", c, "\nAccuracy: ", a)
```
---------------------------------------------------------------------
## lec06-1. Softmax Regression
- 여러개의 클래스가 있을 때 예측하는 Multinomial classification중 가장 많이 사용되는 softmax classification
- Multinomial classification
> A,B,C가 있을 때 3개의 classify를 가짐
> ⅰ) A or not A
> ⅱ) B or not B
> ⅲ) C or not C
> → 3개의 logistic classifier값(Y)을 sigmoid function에 넣으면 0에서 1사이의 Z값이 나옴 
 → Softmax algoirthm = 위의 방식을 통해 나온 값의 합이 1이 되도록 조정하는 것 (=확률)
 → Softmax(a) = Za / (Za+Zb+Zc)
 
## lec06-2. Softmax classifier의 cost function
- entropy :복잡도, 무질서량, entropy가 크다 = 복잡하다  
  → entropy가 감소하는 값 즉 예측값과 실제값의 거리가 감소하는 방향으로 값을 변경하다 보면 최저점을 찾을 수 있게 됨  
- S(Y)=softmax가 예측한 값, L(Y)=실제 Y의 값=lable  
![image](https://user-images.githubusercontent.com/54131109/74914960-badffd80-5406-11ea-885d-cc8da3595ec7.png)

- cross entropy vs logistic cost  
![image](https://user-images.githubusercontent.com/54131109/74915331-612c0300-5407-11ea-9d99-797af1ded3b9.png)
![image](https://user-images.githubusercontent.com/54131109/74915162-14e0c300-5407-11ea-9289-325ed3c76db4.png)
> logistic에서 y=0 or y=1인데 이를 one-hot encoding벡터로 바꾸면 0 → log(1-H(x))이므로 [1,0], 1 → log(H(x))이므로 [0,1]
> corss entropy로 풀면 sigma(Li * -log(Si)) y = L, H(x) = S 이므로  
> L:[0, 1], S:H(x), sigma([0, 1] ( * ) -log[0, 1]) = 0  
> L:[1, 0], S:1-H(x), sigma([1, 0] ( * ) -log[1-0, 1-1]) = sigma([1,0] ( * ) -log[1,0]) = 0  
> 따라서 cross entropy와 logistic cost는 같은 의미
