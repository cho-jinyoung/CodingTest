## lec05-1. Logistic Classification의 가설 함수 정의
- Binary Classification(이항분류)
> ex) spam detection: spam(1) or Ham(0)  
>  Facebook feed: show(1) or hide(0)
- Logistic regression classification의 가설함수(Hypothesis)  
  -z=WX, H(x)=g(z) (0<g(z)<1)  
![시그모이드함수]https://mblogthumb-phinf.pstatic.net/20160426_162/cattree_studio_1461614301227SqDwM_PNG/SigmoidGraph.png?type=w800  
![image](https://user-images.githubusercontent.com/54131109/74749379-6c6b1b80-52ad-11ea-8fc6-dc528cd7f652.png)  
## lec05-2. Logistic Regression의 cost함수 설명  
- New cost function for logistic  
> ⅰ) y=1, H(x)이 1이 나와야 하므로 1에서 멀어질수록 cost값이 커지고 1이 나오면 cost=0  
> ⅱ) y=0, H(x)이 0이 나와야 하므로 0에서 멀어질수록 cost값이 커지고 0이 나오면 cost=0  
![image](https://user-images.githubusercontent.com/54131109/74749942-46924680-52ae-11ea-8489-a44c0af8e06f.png)  
> ⅰ) y=1, -log(H(x)) x값이 1이면 y=0이고 x값이 0에 가까워 지면 y값은 무한에 가까워 짐  
> ⅱ) y=0, -log(1-H(x)) x값이 0이면 y=0이고 x값이 1에 가까워 지면 y값은 무한에 가까워 짐  
> → y값이 cost값이므로 비용 즉 cost값이 최소가 되어야 하므로 y값이 무한에 가까워지는 값은 선택할 수 없음
![image](https://user-images.githubusercontent.com/54131109/74914384-b2d38e00-5405-11ea-90e8-25da554d6c68.png)
- 위의 수식을 경우의 수 없이 하나의 식으로 수식 수정  
![image](https://user-images.githubusercontent.com/54131109/74750815-b05f2000-52af-11ea-845d-c8536f26cfe2.png)  
```python
# cost function
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis)))
```
- Minimize cost-Gradient decent algorithm  
![image](https://user-images.githubusercontent.com/54131109/74751724-097b8380-52b1-11ea-9ec1-ed3048a11aef.png)
```python
a = tf.Variable(0.1)  # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```
------------------------------------------------
## lab05. Tensorflow로 Logistic Classification 구현하기
- training data
```python
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
# placeholders for a tensor that will be always fed.
X = tf.placeholder(tf.float32, shape=[None, 2])
Y = tf.placeholder(tf.float32, shape=[None, 1])
```
```python
W = tf.Variable(tf.random_normal([2, 1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')
# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))
hypothesis = tf.sigmoid(tf.matmul(X, W) + b)
# cost/loss function
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
# Accuracy computation
# True if hypothesis>0.5 else False
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))
```
- train the model
```
# Launch graph
with tf.Session() as sess:
   # Initialize TensorFlow variables
   sess.run(tf.global_variables_initializer())
   for step in range(10001):
       cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
       if step % 200 == 0:
           print(step, cost_val)
   # Accuracy report
   h, c, a = sess.run([hypothesis, predicted, accuracy],
                      feed_dict={X: x_data, Y: y_data})
   print("\nHypothesis: ", h, "\nCorrect (Y): ", c, "\nAccuracy: ", a)
```
---------------------------------------------------------------------
## lec06-1. Softmax Regression
- 여러개의 클래스가 있을 때 예측하는 Multinomial classification중 가장 많이 사용되는 softmax classification
- Multinomial classification
> A,B,C가 있을 때 3개의 classify를 가짐
> ⅰ) A or not A
> ⅱ) B or not B
> ⅲ) C or not C
> → 3개의 logistic classifier값(Y)을 sigmoid function에 넣으면 0에서 1사이의 Z값이 나옴 
 → Softmax algoirthm = 위의 방식을 통해 나온 값의 합이 1이 되도록 조정하는 것 (=확률)
 → Softmax(a) = Za / (Za+Zb+Zc)
 
## lec06-2. Softmax classifier의 cost function
- entropy :복잡도, 무질서량, entropy가 크다 = 복잡하다  
  → entropy가 감소하는 값 즉 예측값과 실제값의 거리가 감소하는 방향으로 값을 변경하다 보면 최저점을 찾을 수 있게 됨  
- S(Y)=softmax가 예측한 값, L(Y)=실제 Y의 값=lable  
![image](https://user-images.githubusercontent.com/54131109/74914960-badffd80-5406-11ea-885d-cc8da3595ec7.png)

- cross entropy vs logistic cost  
![image](https://user-images.githubusercontent.com/54131109/74915331-612c0300-5407-11ea-9d99-797af1ded3b9.png)
![image](https://user-images.githubusercontent.com/54131109/74915162-14e0c300-5407-11ea-9289-325ed3c76db4.png)
> logistic에서 y=0 or y=1인데 이를 one-hot encoding벡터로 바꾸면   
> 0 → log(1-H(x)) => [1,0], 1 → log(H(x)) => [0,1]
> corss entropy로 풀면 sigma(Li * -log(Si)) y = L, H(x) = S 이므로  
> L:[0, 1], S:H(x), sigma([0, 1] ( * ) -log[0, 1]) = 0  
> L:[1, 0], S:1-H(x), sigma([1, 0] ( * ) -log[1-0, 1-1]) = sigma([1,0] ( * ) -log[1,0]) = 0  
> 따라서 cross entropy와 logistic cost는 같은 의미
------------------------------------------------
## lab06-1. Tensorflow로 Softmax Classification구현
- cost function: cross entropy
```python
# Cross entropy cost/loss
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)
```
```python
import tensorflow as tf
tf.set_random_seed(777)  # for reproducibility

x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]
# y_data는 one-hot encoding으로 값을 넣음
y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]

X = tf.placeholder("float", [None, 4])
Y = tf.placeholder("float", [None, 3])
# y의 개수=레이블의 개수=클래스 개수
nb_classes = 3

W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')
b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

# tf.nn.softmax computes softmax activations
# softmax = exp(logits) / reduce_sum(exp(logits), dim)
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

# Cross entropy cost/loss
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

# Launch graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(2001):
            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})

            if step % 200 == 0:
                print(step, cost_val)
# Testing & One-hot encoding
    print('--------------')
    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})
    # argmax = tensorflow함수, softmax의 결과값으로 몇번째의 arg가 가장 큰가? → 1로 반환
    print(a, sess.run(tf.argmax(a, 1)))
# 결과: a=[[1.3890490e-03 9.9860185e-01 9.0613084e-06]] → [1]
    print('--------------')
    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})
    print(b, sess.run(tf.argmax(b, 1)))
# 결과: [[0.9311919  0.06290216 0.00590591]] → [0]
    print('--------------')
    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})
    print(c, sess.run(tf.argmax(c, 1)))
# 결과: [[1.2732815e-08 3.3411323e-04 9.9966586e-01]] → [2]
    print('--------------')
    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})
    print(all, sess.run(tf.argmax(all, 1)))
# 따라서 결과 = [1, 0, 2]
``` 
![image](https://user-images.githubusercontent.com/54131109/75456015-56501000-59bd-11ea-9e29-0cdd58466786.png)
