## lec07-1. Application&tips: Learning rate, data preprocessing, overfitting
### Learning rate
- Large learning rate -overshooting
- Small learning rate -takes too long, stops at local minimum
**=> Try several learning rates → 보통 0.01로 시작 → 발산하면 값을 더 작게, 너무 작게 움직이면 값을 더 크게 수정하며 cost function출력 확인**
### Data(x) preprocessing for gradient descent  
![image](https://user-images.githubusercontent.com/54131109/75787336-ea4d1d80-5da9-11ea-8534-fa765769e1dd.png)  
- zero-centered data → 데이터의 중심이 0으로 가도록
- normalized data → 데이터들이 정해진 범위내에 들어가도록
- standardization (python코드)
![image](https://user-images.githubusercontent.com/54131109/75787622-52036880-5daa-11ea-8667-feb4905317e4.png)  
### overfitting
- 테스트 데이터에는 잘 맞지만 실제 데이터에는 안맞는 경우가 있음  
![image](https://user-images.githubusercontent.com/54131109/75788204-3e0c3680-5dab-11ea-9c28-20ba3da61b81.png)  
 → model1이 더 좋은 모델, model2는 저 데이터에만 맞춰진 모델  
**=> more training data, reduce the number of features, Regularization**  
#### Regularization(일반화)
- cost함수에 각 엘리먼트의 값의 합(=regularization)을 추가
```python
# regularization
l2reg = 0.001 * tf.reduce_sum(tf.square(W))
```
------------------
## lab.07-1. training/test dataset, learning rate, normalization
```python
import tensorflow as tf
tf.set_random_seed(777)  # for reproducibility

#training dataset
x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]
y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]

#test dataset
x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]
y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]

X = tf.placeholder("float", [None, 3])
Y = tf.placeholder("float", [None, 3])
W = tf.Variable(tf.random_normal([3, 3]))
b = tf.Variable(tf.random_normal([3]))

# tf.nn.softmax computes softmax activations
# softmax = exp(logits) / reduce_sum(exp(logits), dim)
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

# Cross entropy cost/loss
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
# Try to change learning_rate to small numbers
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

# Correct prediction Test model 값을 예측하고 맞는지 아닌지 확인
prediction = tf.argmax(hypothesis, 1)
is_correct = tf.equal(prediction, tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

# Launch graph
with tf.Session() as sess:
    # Initialize TensorFlow variables
    sess.run(tf.global_variables_initializer())
    for step in range(201):
        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})
        print(step, cost_val, W_val)
    # predict
    print("Prediction:", sess.run(prediction, feed_dict={X: x_test}))
    # Calculate the accuracy
    print("Accuracy: ", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))
```
![image](https://user-images.githubusercontent.com/54131109/75792170-ef619b00-5db0-11ea-843d-7a89ea917aa9.png)  
** Prediction, Accuracy값은 test data를 넣었을 때의 값(맨 앞의 숫자가 step)** 
- learning rate: NaN
![image](https://user-images.githubusercontent.com/54131109/75791829-7b26f780-5db0-11ea-95f7-9b87a7c3ede2.png)  
```python
# Cross entropy cost/loss
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
# learning_rate의 값만 크게 변경
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)
```
**cost의 결과가 [nan nan nan]으로 나올 경우 learning_rate이 크다고 의심**  
**step이 반복되는데도 cost의 값이 같은 경우 learning_rate이 너무 작아 local minimam에 빠졌거나 거의 이동되지 않는 것 **
