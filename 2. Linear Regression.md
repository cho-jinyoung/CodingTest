## lec02. Linear Regression의 Hypothesis와 cost설명
- regression:회귀, 지도학습의 종류로 예측변수(input)와 연속적인 반응변수(output)이 주어졌을 때 출력값을 예측하는 두 변수 사이의 관계 찾음  
- **H(x)=ax+b**  
- cost function(loss function):실제 데이터(y)와 hypothesis(H(x))간의 차를 나타내는 것  
## lab02. Tensorflow로 간단한 linear regression구현
1. build graph
```py
# H(x) = Wx + b >>W(weigh)과 b(bias)를 정의해야 함

# X and Y data
x_train = [1, 2, 3] //input
y_train = [1, 2, 3] //output
# Try to find values for W and b to compute y_data = x_data * W + b
# We know that W should be 1 and b should be 0
# But let TensorFlow figure it out
#tensorflow에서 회귀 노드=Variable, tensorflow학습과정에서 변경가능한 값을 의미함
W = tf.Variable(tf.random_normal([1]), name="weight") 
b = tf.Variable(tf.random_normal([1]), name="bias") 
# Our hypothesis XW+b
hypothesis = x_train * W + b
# cost/loss function
# t=[1,2,3]일때 tf.reduce_mean(t)==>2
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
# GradientDescent -Cost를 minimize하는 방법
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train=optimizer.minimize(cost)
# train →그래프에서 최상위 노드
```py
2. Run/update graph and get results
# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())
# Fit the line
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))
```
>  cost      W            b
> 0 2.82329 [ 2.12867713] [-0.85235667]
> 20 0.190351 [ 1.53392804] [-1.05059612]
> 40 0.151357 [ 1.45725465] [-1.02391243]
> ...
> 1960 1.46397e-05 [ 1.004444] [-0.01010205]
> 1980 1.32962e-05 [ 1.00423515] [-0.00962736]
> 2000 1.20761e-05 [ 1.00403607] [-0.00917497]

- placeholders:직접 값을 주지 않고 a,b와 같은 placeholder노드를 생성하고 필요할 때 값을 줌
```py
# placeholders for a tensor that will be always fed using feed_dict
X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None]
```
