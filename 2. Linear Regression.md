## lec02. Linear Regression의 Hypothesis와 cost설명
- regression:회귀, 지도학습의 종류로 예측변수(input)와 연속적인 반응변수(output)이 주어졌을 때 출력값을 예측하는 두 변수 사이의 관계 찾음  
- **H(x)=ax+b**  
- cost function(loss function):실제 데이터(y)와 hypothesis(H(x))간의 차를 나타내는 것  
## lab02. Tensorflow로 간단한 linear regression구현
1. build graph
```python
# H(x) = Wx + b >>W(weigh)과 b(bias)를 정의해야 함  

# X and Y data
x_train = [1, 2, 3] //input
y_train = [1, 2, 3] //output
# Try to find values for W and b to compute y_data = x_data * W + b
# We know that W should be 1 and b should be 0
# But let TensorFlow figure it out
#tensorflow에서 회귀 노드=Variable, tensorflow학습과정에서 변경가능한 값을 의미함
W = tf.Variable(tf.random_normal([1]), name="weight") 
b = tf.Variable(tf.random_normal([1]), name="bias") 
# Our hypothesis XW+b
hypothesis = x_train * W + b
# cost/loss function
# t=[1,2,3]일때 tf.reduce_mean(t)==>2
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
# GradientDescent -Cost를 minimize하는 방법
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train=optimizer.minimize(cost)
# train →그래프에서 최상위 노드
```
2. Run/update graph and get results
```python
# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())
# Fit the line
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))
```
| - | cost | W | b
|----|-----|---|---
| 0 | 2.82329 | [ 2.12867713] | [-0.85235667] 
| 20 | 0.190351 | [ 1.53392804] | [-1.05059612]
| 40 | 0.151357 | [ 1.45725465] | [-1.02391243] 
|----|-----|---|---
| 1960 | 1.46397e-05 | [ 1.004444] | [-0.01010205] 
| 1980 | 1.32962e-05 | [ 1.00423515] | [-0.00962736]  
| 2000 | 1.20761e-05 | [ 1.00403607] | [-0.00917497]  
- placeholders:직접 값을 주지 않고 a,b와 같은 placeholder노드를 생성하고 필요할 때 값을 줌
```python
# placeholders for a tensor that will be always fed using feed_dict
X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])
```
3. update variables in the graph(and return values) 
-------------------------------------------
## lec03. Linear Regression의 cost최소화 알고리즘의 원리 설명
- cost function  
H(x) → prediction, y → true
![image](https://user-images.githubusercontent.com/54131109/74310226-0adc1600-4db0-11ea-8834-c993725b3b6c.png)
![image](https://user-images.githubusercontent.com/54131109/74310469-a2d9ff80-4db0-11ea-9e32-a6832823bd10.png)
- cost가 최소인 즉 0인 W값을 찾아야 함  
> Minimize cost function → Gradient descent algorithm(경사하강법)  
> - 어떤 값에서든 시작 가능, 경사도 계산을 통해 W값을 바꾸며 cost최솟값을 찾음  
> - cost function의 기울기 ![image](https://user-images.githubusercontent.com/54131109/74311119-07498e80-4db2-11ea-8f2a-2f0be77483da.png)  
> - convex function:cost function설계시 convex function의 형태가 되어야 경사하강법을 사용할 때 오류가 없음
## lab03. Linear Regression의 cost최소화의 Tensorflow구현
```python
# cost function 시각화
import tensorflow as tf
import matplotlib.pyplot as plt
X = [1, 2, 3]
Y = [1, 2, 3]
W = tf.placeholder(tf.float32)
# Our hypothesis for linear model H(x) = Wx
hypothesis = X * W
# cost/loss function -차를 제곱한다음 평균
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Launch the graph in a session.
sess = tf.Session()
# Variables for plotting cost function
W_history = []
cost_history = []
# Initialized global variables in the graph
# Variables for plotting cost function
W_val = []
cost_val = []
for i in range(-30, 50):
    feed_W = i * 0.1
    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: curr_W})
    W_val.append(curr_W)
    cost_val.append(curr_cost)
# Show the cost function
plt.plot(W_history, cost_history)
plt.show()
```
```python
# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative
# learning_rate=알파값 
learning_rate = 0.1
gradient = tf.reduce_mean((W * X - Y) * X)
descent = W - learning_rate * gradient
# tensor에서는 바로 assign이 불가능하므로 .assign이라는 함수를 통해 update에 값 assign, 이후 update를 그래프에서 실행시키면 동작 
update = W.assign(descent)
```
```python
# Minimize: Gradient Descent Optimizer = cost값을 미분하지 않아도 경사하강법 가능, 위와 같은 역할을 하는 코드
train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)
```
##lec04. multi-variable linear regression 
- H(x1,x2,x3..)=w1x1+w2x2+w3x3.. →Hypothesis using Matrix H(X)=XW
![image](https://user-images.githubusercontent.com/54131109/74604398-f2962f00-5100-11ea-96a5-c8b17b984348.png)
- 인스턴스가 많을 때
![image](https://user-images.githubusercontent.com/54131109/74604440-620c1e80-5101-11ea-819b-e43bf23e14e0.png)
![image](https://user-images.githubusercontent.com/54131109/74604444-6e907700-5101-11ea-8500-4dc224af437d.png)
##lab04. multi-variable linear regression을 tensorflow에서 구현하기, tensorflow로 파일에서 데이터 읽어오기
```python
import tensorflow as tf
x_data = [[73., 80., 75.], [93., 88., 93.],
         [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]
y_data = [[152.], [185.], [180.], [196.], [142.]]
# placeholders for a tensor that will be always fed.
X = tf.placeholder(tf.float32, shape=[None, 3])
Y = tf.placeholder(tf.float32, shape=[None, 1])

W = tf.Variable(tf.random_normal([3, 1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Hypothesis
hypothesis = tf.matmul(X, W) + b
# Simplified cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())

for step in range(2001):
   cost_val, hy_val, _ = sess.run(
       [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
   if step % 10 == 0:
       print(step, "Cost: ", cost_val, "\nPrediction:\n", hy_val)
```
```python
# Numpy를 이용하여 파일에서 데이터 읽어오기
import numpy as np
xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)
# 전체 n행의 첫번째부터 마지막 이전 열까지의 데이터 = x
x_data = xy[:, 0:-1]
# 전체 n행의 마지막 열 데이터 = y
y_data = xy[:, [-1]]
# Make sure the shape and data are OK
print(x_data.shape, x_data, len(x_data))
print(y_data.shape, y_data)
```
- Queue Runner → 여러개의 파일을 관리하기 위해
![image](https://user-images.githubusercontent.com/54131109/74606027-5ecc5f00-5110-11ea-9086-6e039e25e16d.png)
1. 파일이름 리스트 생성  
```python
filename_queue=tf.train.string_input_producer(['data-01.csv', 'data-02', ..], shuffle=False, name='filename_queue')  
```
2. 텍스트 파일 읽어옴  
```python
reader=tf.TextLineReader()  
key, value=reader.read(filename_queue)
```
3. 값의 default값, 자료형 지정  
```python
# Default values, in case of empty columns. Also specifies the type of the
# decoded result.
record_defaults=[[0.], [0.], [0.], [0.]]  
xy=tf.decode_csv(value, record_defaults=record_defaults)  
```
