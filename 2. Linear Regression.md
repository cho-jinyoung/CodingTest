## lec02. Linear Regression의 Hypothesis와 cost설명
- regression:회귀, 지도학습의 종류로 예측변수(input)와 연속적인 반응변수(output)이 주어졌을 때 출력값을 예측하는 두 변수 사이의 관계 찾음  
- **H(x)=ax+b**  
- cost function(loss function):실제 데이터(y)와 hypothesis(H(x))간의 차를 나타내는 것  
## lab02. Tensorflow로 간단한 linear regression구현
1. build graph
```python
# H(x) = Wx + b >>W(weigh)과 b(bias)를 정의해야 함  

# X and Y data
x_train = [1, 2, 3] //input
y_train = [1, 2, 3] //output
# Try to find values for W and b to compute y_data = x_data * W + b
# We know that W should be 1 and b should be 0
# But let TensorFlow figure it out
#tensorflow에서 회귀 노드=Variable, tensorflow학습과정에서 변경가능한 값을 의미함
W = tf.Variable(tf.random_normal([1]), name="weight") 
b = tf.Variable(tf.random_normal([1]), name="bias") 
# Our hypothesis XW+b
hypothesis = x_train * W + b
# cost/loss function
# t=[1,2,3]일때 tf.reduce_mean(t)==>2
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
# GradientDescent -Cost를 minimize하는 방법
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train=optimizer.minimize(cost)
# train →그래프에서 최상위 노드
```
2. Run/update graph and get results
```python
# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())
# Fit the line
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))
```
| - | cost | W | b
|----|-----|---|---
| 0 | 2.82329 | [ 2.12867713] | [-0.85235667] 
| 20 | 0.190351 | [ 1.53392804] | [-1.05059612]
| 40 | 0.151357 | [ 1.45725465] | [-1.02391243] 
|----|-----|---|---
| 1960 | 1.46397e-05 | [ 1.004444] | [-0.01010205] 
| 1980 | 1.32962e-05 | [ 1.00423515] | [-0.00962736]  
| 2000 | 1.20761e-05 | [ 1.00403607] | [-0.00917497]  
- placeholders:직접 값을 주지 않고 a,b와 같은 placeholder노드를 생성하고 필요할 때 값을 줌
```python
# placeholders for a tensor that will be always fed using feed_dict
X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])
```
3. update variables in the graph(and return values) 
-------------------------------------------
## lec03. Linear Regression의 cost최소화 알고리즘의 원리 설명
![image](https://user-images.githubusercontent.com/54131109/74310226-0adc1600-4db0-11ea-8834-c993725b3b6c.png)
![image](https://user-images.githubusercontent.com/54131109/74310469-a2d9ff80-4db0-11ea-9e32-a6832823bd10.png)
- cost가 최소인 즉 0인 W값을 찾아야 함  
> Minimize cost function → Gradient descent algorithm(경사하강법)  
> - 어떤 값에서든 시작 가능, 경사도 계산을 통해 W값을 바꾸며 cost최솟값을 찾음  
> - cost function의 기울기 ![image](https://user-images.githubusercontent.com/54131109/74311119-07498e80-4db2-11ea-8f2a-2f0be77483da.png)  
> - convex function:cost function설계시 convex function의 형태가 되어야 경사하강법을 사용할 때 오류가 없음
## lab03. Linear Regression의 cost최소화의 Tensorflow구현
```python
# cost function 시각화
import tensorflow as tf
import matplotlib.pyplot as plt
X = [1, 2, 3]
Y = [1, 2, 3]
W = tf.placeholder(tf.float32)
# Our hypothesis for linear model H(x) = Wx
hypothesis = X * W
# cost/loss function -차를 제곱한다음 평균
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Launch the graph in a session.
sess = tf.Session()
# Variables for plotting cost function
W_history = []
cost_history = []
# Initialized global variables in the graph
# Variables for plotting cost function
W_val = []
cost_val = []
for i in range(-30, 50):
    feed_W = i * 0.1
    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: curr_W})
    W_val.append(curr_W)
    cost_val.append(curr_cost)
# Show the cost function
plt.plot(W_history, cost_history)
plt.show()
```
```python
# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative
# learning_rate=알파값 
learning_rate = 0.1
gradient = tf.reduce_mean((W * X - Y) * X)
descent = W - learning_rate * gradient
# tensor에서는 바로 assign이 불가능하므로 .assign이라는 함수를 통해 update에 값 assign, 이후 update를 그래프에서 실행시키면 동작 
update = W.assign(descent)
```
```python
# Minimize: Gradient Descent Optimizer = cost값을 미분하지 않아도 경사하강법 가능, 위와 같은 역할을 하는 코드
train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)
```
